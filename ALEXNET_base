# 1. loading data

import os
from glob import glob
import random
import time
import tensorflow
import datetime
os.environ['KERAS_BACKEND'] = 'tensorflow'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 3 = INFO, WARNING, and ERROR
from tqdm import tqdm
import numpy as np
import pandas as pd
from IPython.display import FileLink
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns 
%matplotlib inline
from IPython.display import display, Image
import matplotlib.image as mpimg
import cv2

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_files       
from tensorflow.keras.utils import to_categorical
from sklearn.utils import shuffle
from sklearn.metrics import log_loss

from tensorflow import keras 

from keras.models import Sequential, Model
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.applications.vgg16 import VGG16


dataset = pd.read_csv('/kaggle/input/state-farm-distracted-driver-detection/driver_imgs_list.csv')
dataset.head(5)




# 2. 수치 Groupby subjects
by_drivers = dataset.groupby('subject') 
# Groupby unique drivers
unique_drivers = by_drivers.groups.keys() # drivers id
print('There are : ',len(unique_drivers), ' unique drivers')
print('There is a mean of ',round(dataset.groupby('subject').count()['classname'].mean()), ' images by driver.')



# 3. 각각의 클래스 이미지 예시 출력

NUMBER_CLASSES = 10 # 10 classeslinkcode
activity_map = {'c0': 'Safe driving', 
                'c1': 'Texting - right', 
                'c2': 'Talking on the phone - right', 
                'c3': 'Texting - left', 
                'c4': 'Talking on the phone - left', 
                'c5': 'Operating the radio', 
                'c6': 'Drinking', 
                'c7': 'Reaching behind', 
                'c8': 'Hair and makeup', 
                'c9': 'Talking to passenger'}


plt.figure(figsize = (12, 20))
image_count = 1
BASE_URL = '../input/state-farm-distracted-driver-detection/imgs/train/'
for directory in os.listdir(BASE_URL):
    if directory[0] != '.':
        for i, file in enumerate(os.listdir(BASE_URL + directory)):
            if i == 1:
                break
            else:
                fig = plt.subplot(5, 2, image_count)
                image_count += 1
                image = mpimg.imread(BASE_URL + directory + '/' + file)
                plt.imshow(image)
                plt.title(activity_map[directory])

# 4. 이미지제너레이터를 이용한 데이터셋 전처리
from tensorflow.keras.preprocessing.image import ImageDataGenerator

BASE_URL = '../input/state-farm-distracted-driver-detection/imgs/train/'

train_datagen = ImageDataGenerator(
    rescale=1./255, #픽셀값을 0-1 범위로
    validation_split=0.2 #20%를 검증 데이터로
)

#학습데이터 생성
train_generator = train_datagen.flow_from_directory(
    directory=BASE_URL,
    target_size=(227,227), #alexnet 입력크기
    batch_size = 32,
    class_mode='categorical',
    subset='training'
)

#검증데이터 생성
validation_generator = train_datagen.flow_from_directory(
    directory=BASE_URL,
    target_size=(227,227), 
    batch_size = 32,
    class_mode='categorical',
    subset='validation'
)

#5. AlexNet
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

model = Sequential([
  Conv2D(96,(11,11), strides=4, activation='relu', input_shape=(227,227,3)), 
  MaxPooling2D(pool_size=(3,3), strides=2),

  Conv2D(256,(5,5), strides=1, activation='relu'), 
  MaxPooling2D(pool_size=(3,3), strides=2),

  Conv2D(384,(3,3), padding='same', activation='relu'), 
  Conv2D(384,(3,3), padding='same', activation='relu'), 
  Conv2D(256,(3,3), padding='same', activation='relu'), 

  Flatten(),
    
  Dense(1024, activation='relu'),
  Dropout(0.5),

  Dense(NUMBER_CLASSES, activation='softmax')
]
)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

history = model.fit(
    train_generator,
    epochs=10,
    validation_data=validation_generator,
    steps_per_epoch=train_generator.samples // 32,
    validation_steps=validation_generator.samples // 32
)
